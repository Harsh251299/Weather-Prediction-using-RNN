{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y1GrH3BiiNZ"
      },
      "source": [
        "\n",
        "# Weather Prediction using RNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApF3CyQ6UYT9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sympy import diff, symbols, exp\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tLIqnn8UjZ0"
      },
      "outputs": [],
      "source": [
        "# Activation function\n",
        "def Tanh(xVar):\n",
        "    return np.tanh(xVar)\n",
        "\n",
        "def Loss(actualsVal, predictionsVal):\n",
        "    return np.mean(((actualsVal -  predictionsVal))**2)\n",
        "\n",
        "def derivationOfLoss(actualsVal, predictionsVal):\n",
        "    return (predictionsVal - actualsVal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGXmIbKLU04u"
      },
      "outputs": [],
      "source": [
        "# class of Neural Network\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, data, noOfAttribute, hiddenNodes, noOfOutput, activationFunction, learningRate, Loss, derivationOfLoss) -> None:\n",
        "        self.data = data\n",
        "        self.activationFunction = activationFunction\n",
        "        self.learningRate = learningRate\n",
        "        self.Loss = Loss\n",
        "        self.derivationOfLoss = derivationOfLoss\n",
        "\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.Xtrain = None\n",
        "        self.Xtest = None\n",
        "        self.Ytrain = None\n",
        "        self.Ytest = None\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        self.layer_conf = [\n",
        "                              {\"type\":\"input\", \"units\": noOfAttribute},\n",
        "                              {\"type\": \"rnn\", \"hidden\": hiddenNodes, \"output\": noOfOutput}\n",
        "                          ]\n",
        "\n",
        "    # Data preprocessing methods\n",
        "    def normalize(self):\n",
        "            self.data[self.inputs] = self.scaler.fit_transform(data[self.inputs])\n",
        "            self.data[self.outputs] = self.scaler.fit_transform(data[self.outputs])\n",
        "\n",
        "\n",
        "    def printCorrelationMatrix(self):\n",
        "            # Cleaning the data\n",
        "            self.dataCleaning()\n",
        "            # establishing a relationship between each characteristic\n",
        "            matrixOfCorrelation = self.data.corr()\n",
        "            plt.figure(figsize=(9, 9))\n",
        "            sns.heatmap(matrixOfCorrelation, annot=True, cmap=\"coolwarm\", fmt='.2f', square=True)\n",
        "            plt.title('Correlation Matrix of Numeric Features')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def dataCleaning(self):\n",
        "            print(\"Null values exist\") if self.data.isna().any().any() else print(\"Not a single null value exists\")\n",
        "            self.data['date'] = pd.to_datetime(self.data['date'], dayfirst=True)\n",
        "\n",
        "            self.data['year'] = self.data['date'].dt.year\n",
        "            self.data['month'] = self.data['date'].dt.month\n",
        "            self.data['day'] = self.data['date'].dt.day\n",
        "            self.data = self.data.drop(\"date\", axis=1)\n",
        "\n",
        "\n",
        "            # Drop the original categorical column\n",
        "            self.data.drop('weather', axis=1, inplace=True)\n",
        "            self.data.drop('tmax_tomorrow', axis=1, inplace=True)\n",
        "\n",
        "            self.inputs = ['precipitation', 'temp_max', 'temp_min', 'wind', 'year','month','day']\n",
        "            self.outputs = ['tmax_tomorrow']\n",
        "\n",
        "            self.normalize()\n",
        "\n",
        "\n",
        "     # The entire set of data will be divided into training and testing sets using this function.\n",
        "    def split(self, percentageOfSplit):\n",
        "            np.random.seed(0)\n",
        "            train_size = int(percentageOfSplit * len(self.data))\n",
        "            self.Xtrain = self.data[self.inputs].iloc[:train_size].to_numpy()\n",
        "            self.Ytrain = self.data[self.outputs].iloc[:train_size].to_numpy()\n",
        "            self.Xtest = self.data[self.inputs].iloc[train_size:].to_numpy()\n",
        "            self.Ytest = self.data[self.outputs].iloc[train_size:].to_numpy()\n",
        "\n",
        "    def init_params(self):\n",
        "            layers = []\n",
        "            for i in range(1, len(self.layer_conf)):\n",
        "                np.random.seed(0)\n",
        "                k = 1/math.sqrt(self.layer_conf[i][\"hidden\"])\n",
        "                i_weight = np.random.rand(self.layer_conf[i-1][\"units\"], self.layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "\n",
        "                h_weight = np.random.rand(self.layer_conf[i][\"hidden\"], self.layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "                h_bias = np.random.rand(1, self.layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "\n",
        "                o_weight = np.random.rand(self.layer_conf[i][\"hidden\"], self.layer_conf[i][\"output\"]) * 2 * k - k\n",
        "                o_bias = np.random.rand(1, self.layer_conf[i][\"output\"]) * 2 * k - k\n",
        "\n",
        "                layers.append(\n",
        "                    [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
        "                )\n",
        "            return layers\n",
        "\n",
        "    def forwardPass(self, x, layers):\n",
        "            hiddens = []\n",
        "            outputs = []\n",
        "            for i in range(len(layers)):\n",
        "                i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
        "                hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
        "                output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
        "                for j in range(x.shape[0]):\n",
        "                    input_x = x[j,:][np.newaxis,:] @ i_weight\n",
        "                    hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
        "                    # Activation.  tanh avoids outputs getting larger and larger.\n",
        "                    hidden_x = self.activationFunction(hidden_x)\n",
        "                    # Store hidden for use in backprop\n",
        "                    hidden[j,:] = hidden_x\n",
        "\n",
        "                    # Output layer\n",
        "                    output_x = hidden_x @ o_weight + o_bias\n",
        "                    output[j,:] = output_x\n",
        "                hiddens.append(hidden)\n",
        "                outputs.append(output)\n",
        "            return hiddens, outputs[-1]\n",
        "\n",
        "    def predict(self, dataInput, layers):\n",
        "        _,predictions = self.forwardPass(dataInput,layers)\n",
        "        return predictions\n",
        "\n",
        "    def backwardPass(self, layers, x, lr, grad, hiddens):\n",
        "            for i in range(len(layers)):\n",
        "                i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
        "                hidden = hiddens[i]\n",
        "                next_h_grad = None\n",
        "                i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
        "\n",
        "                for j in range(x.shape[0] - 1, -1, -1):\n",
        "                    # Add newaxis in the first dimension\n",
        "                    out_grad = grad[j,:][np.newaxis, :]\n",
        "\n",
        "                    # Output updates\n",
        "                    # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
        "                    o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
        "                    o_bias_grad += out_grad\n",
        "\n",
        "                    # Propagate gradient to hidden unit\n",
        "                    h_grad = out_grad @ o_weight.T\n",
        "\n",
        "                    if j < x.shape[0] - 1:\n",
        "                        # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
        "                        hh_grad = next_h_grad @ h_weight.T\n",
        "                        # Add the gradients together to combine output contribution and hidden contribution\n",
        "                        h_grad += hh_grad\n",
        "\n",
        "                    # Pull the gradient across the current hidden nonlinearity\n",
        "                    # derivative of tanh is 1 - tanh(x) ** 2\n",
        "                    # So we take the output of tanh (next hidden state), and plug in\n",
        "                    tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
        "\n",
        "                    # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
        "                    # Effect is to pull value across nonlinearity\n",
        "                    h_grad = np.multiply(h_grad, tanh_deriv)\n",
        "\n",
        "                    # Store to compute h grad for previous sequence position\n",
        "                    next_h_grad = h_grad.copy()\n",
        "\n",
        "                    # If we're not at the very beginning\n",
        "                    if j > 0:\n",
        "                        h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
        "                        h_bias_grad += h_grad\n",
        "\n",
        "                    i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
        "\n",
        "                # Normalize lr by number of sequence elements\n",
        "                lr = lr / x.shape[0]\n",
        "                i_weight -= i_weight_grad * lr\n",
        "                h_weight -= h_weight_grad * lr\n",
        "                h_bias -= h_bias_grad * lr\n",
        "                o_weight -= o_weight_grad * lr\n",
        "                o_bias -= o_bias_grad * lr\n",
        "                layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
        "            return layers\n",
        "\n",
        "    def train(self, epochs):\n",
        "        layers = self.init_params()\n",
        "\n",
        "\n",
        "        # Splitting the data into training and testing\n",
        "        self.split(percentageOfSplit=0.8)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            sequence_len = 6\n",
        "            epoch_loss = 0\n",
        "            for j in range(self.Xtrain.shape[0] - sequence_len):\n",
        "                seq_x = self.Xtrain[j:(j+sequence_len),]\n",
        "                seq_y = self.Ytrain[j:(j+sequence_len),]\n",
        "                hiddens, outputs = self.forwardPass(seq_x, layers)\n",
        "                grad = derivationOfLoss(seq_y, outputs)\n",
        "                params = self.backwardPass(layers, seq_x, self.learningRate, grad, hiddens)\n",
        "                epoch_loss += Loss(seq_y, outputs)\n",
        "\n",
        "            if epoch % 50 == 0:\n",
        "                sequence_len = 6\n",
        "                test_loss = 0\n",
        "                for j in range(self.Xtest.shape[0] - sequence_len):\n",
        "                    seq_x = self.Xtest[j:(j+sequence_len),]\n",
        "                    seq_y = self.Ytest[j:(j+sequence_len),]\n",
        "                    _, outputs = self.forwardPass(seq_x, layers)\n",
        "                    test_loss += Loss(seq_y, outputs)\n",
        "\n",
        "      #Model Evaluation:\n",
        "        predictionsOftraining = self.predict(self.Xtrain,layers)\n",
        "        print(\"The following are the train scores:\")\n",
        "        print(f\"Mean Square Error: {mean_squared_error(self.Ytrain, predictionsOftraining)}\")\n",
        "        print(f\"Mean Absolute Error: {mean_absolute_error(self.Ytrain, predictionsOftraining)}\")\n",
        "        print(f\"R square Fit:  {r2_score(self.Ytrain, predictionsOftraining)}\")\n",
        "        print(f\"Loss: {epoch_loss / len(self.Xtrain)}\")\n",
        "\n",
        "        print(\"\\nThe following are the Test scores:\")\n",
        "        predictionsOftesting = self.predict(self.Xtest,layers)\n",
        "        print(f\"Mean Square Error: {mean_squared_error(self.Ytest, predictionsOftesting)}\")\n",
        "        print(f\"Mean Absolute Error: {mean_absolute_error(self.Ytest, predictionsOftesting)}\")\n",
        "        print(f\"R square Fit:  {r2_score(self.Ytest, predictionsOftesting)}\")\n",
        "        print(f\"Loss: {test_loss / len(self.Xtest)}\")\n",
        "\n",
        "    # Scatter Plot of Training Data\n",
        "        plt.figure(figsize=(7, 7))\n",
        "        plt.scatter(self.Ytrain, predictionsOftraining, color='blue', marker='o')\n",
        "        # Diagonal line represents true label\n",
        "        plt.plot([self.Ytrain.min(), self.Ytrain.max()], [self.Ytrain.min(), self.Ytrain.max()], 'k-', lw = 1)\n",
        "        plt.xlabel('True Label')\n",
        "        plt.ylabel('Prediction')\n",
        "        plt.title('Scatter plot of True Label vs. Prediction for Training Data')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # Scatter Plot of Testing Data\n",
        "        plt.figure(figsize=(7, 7))\n",
        "        plt.scatter(self.Ytest, predictionsOftesting, color='blue', marker='o')\n",
        "        # Diagonal line represents true label\n",
        "        plt.plot([self.Ytest.min(), self.Ytest.max()], [self.Ytest.min(), self.Ytest.max()], 'k-', lw = 1)\n",
        "        plt.xlabel('True Label')\n",
        "        plt.ylabel('Prediction')\n",
        "        plt.title('Scatter plot of True Label vs. Prediction for Test Data')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oMswo3TBHq_q",
        "outputId": "a3e4ddfb-dd3e-4dd7-ecdd-8952823b4792"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/Harsh251299/Weather-Prediction-using-RNN/refs/heads/main/seattle-weather.csv\")\n",
        "    data = data.ffill()\n",
        "\n",
        "    NeuralNet = NeuralNetwork(data=data, hiddenNodes=14, noOfAttribute=7, noOfOutput=1, activationFunction=Tanh, learningRate=1e-5, Loss=Loss, derivationOfLoss=derivationOfLoss)\n",
        "    NeuralNet.printCorrelationMatrix()\n",
        "    NeuralNet.train(2000)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
